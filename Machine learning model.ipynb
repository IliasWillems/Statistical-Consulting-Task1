{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3330cb03",
   "metadata": {},
   "source": [
    "# Machine learning model\n",
    "This script tries to predict the data using a machine learning model, namely a multilayer perceptron. Other ideas might be implemented later.\n",
    "\n",
    "As is often a good idea with machine learning, we will split the data into a training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80aab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602baf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('Data//modified_training_data.csv')\n",
    "\n",
    "# Use all data or work on subset?\n",
    "USE_ALL_DATA = False\n",
    "WRITE_DATA = False\n",
    "\n",
    "if not USE_ALL_DATA:\n",
    "    df = df.sample(n=10000, replace=False)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "# Since the data is not fully prepped, we do it here. This section will be moved to Data_modification.ipynb\n",
    "\n",
    "# Remove first index column. This should not be necessary anymore in the new Data_modification.ipynb\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "# Create a 'day' and 'month' column instead of datetimes\n",
    "df['SCHEDULED_DEPARTURE'] = pd.to_datetime(df['SCHEDULED_DEPARTURE'])\n",
    "df['DEPARTURE_DAY'] = df['SCHEDULED_DEPARTURE'].apply(lambda x: x.weekday())\n",
    "df['DEPARTURE_MONTH'] = df['SCHEDULED_DEPARTURE'].apply(lambda x: x.month)\n",
    "df.drop(columns=['SCHEDULED_DEPARTURE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b593bbe",
   "metadata": {},
   "source": [
    "## Construct the training and test sets\n",
    "\n",
    "To construct the training and validation set, we make use of in-build functions in sklearn. __Make sure to take note of the selected target variable!__.\n",
    "\n",
    "As our machine learning model cannot handle categorical variables, we first encode them into a one-hot encoding. Furthermore, we try training the MLP both on standerdized and unstanderdized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2a892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>TOTAL_DELAY</th>\n",
       "      <th>TARGET_1</th>\n",
       "      <th>TARGET_2</th>\n",
       "      <th>DEPARTURE_DAY</th>\n",
       "      <th>DEPARTURE_MONTH</th>\n",
       "      <th>...</th>\n",
       "      <th>DEST_AIR_TVC</th>\n",
       "      <th>DEST_AIR_TXK</th>\n",
       "      <th>DEST_AIR_TYR</th>\n",
       "      <th>DEST_AIR_TYS</th>\n",
       "      <th>DEST_AIR_UST</th>\n",
       "      <th>DEST_AIR_VLD</th>\n",
       "      <th>DEST_AIR_VPS</th>\n",
       "      <th>DEST_AIR_WRG</th>\n",
       "      <th>DEST_AIR_XNA</th>\n",
       "      <th>DEST_AIR_YAK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19:08:00</td>\n",
       "      <td>228.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07:50:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20:45:00</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>371</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17:25:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>872</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10:00:00</td>\n",
       "      <td>145.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>763</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>09:17:00</td>\n",
       "      <td>237.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1635</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>17:50:00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>453</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>16:58:00</td>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>00:15:00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>20:17:00</td>\n",
       "      <td>167.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>762</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SCHEDULED_ARRIVAL  SCHEDULED_TIME  ELAPSED_TIME  DISTANCE  CANCELLED  \\\n",
       "0             19:08:00           228.0         211.0      1400          0   \n",
       "1             07:50:00            80.0          86.0       369          0   \n",
       "2             20:45:00            80.0          76.0       371          0   \n",
       "3             17:25:00           140.0         156.0       872          0   \n",
       "4             10:00:00           145.0         143.0       763          0   \n",
       "...                ...             ...           ...       ...        ...   \n",
       "9995          09:17:00           237.0         219.0      1635          0   \n",
       "9996          17:50:00            95.0          87.0       453          0   \n",
       "9997          16:58:00            52.0          43.0        77          0   \n",
       "9998          00:15:00           105.0          84.0       484          0   \n",
       "9999          20:17:00           167.0           NaN       762          0   \n",
       "\n",
       "      TOTAL_DELAY  TARGET_1  TARGET_2  DEPARTURE_DAY  DEPARTURE_MONTH  ...  \\\n",
       "0            27.0      27.0      27.0              0                6  ...   \n",
       "1           -12.0     -12.0     -12.0              3                5  ...   \n",
       "2            -2.0      -2.0      -2.0              0                5  ...   \n",
       "3            16.0      16.0      16.0              4                6  ...   \n",
       "4            -4.0      -4.0      -4.0              3                4  ...   \n",
       "...           ...       ...       ...            ...              ...  ...   \n",
       "9995          1.0       1.0       1.0              6                6  ...   \n",
       "9996        112.0     112.0     112.0              3                4  ...   \n",
       "9997         -5.0      -5.0      -5.0              6                7  ...   \n",
       "9998         -1.0      -1.0      -1.0              3                7  ...   \n",
       "9999         11.0      11.0      11.0              3                5  ...   \n",
       "\n",
       "      DEST_AIR_TVC  DEST_AIR_TXK  DEST_AIR_TYR  DEST_AIR_TYS  DEST_AIR_UST  \\\n",
       "0                0             0             0             0             0   \n",
       "1                0             0             0             0             0   \n",
       "2                0             0             0             0             0   \n",
       "3                0             0             0             0             0   \n",
       "4                0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "9995             0             0             0             0             0   \n",
       "9996             0             0             0             0             0   \n",
       "9997             0             0             0             0             0   \n",
       "9998             0             0             0             0             0   \n",
       "9999             0             0             0             0             0   \n",
       "\n",
       "      DEST_AIR_VLD  DEST_AIR_VPS  DEST_AIR_WRG  DEST_AIR_XNA  DEST_AIR_YAK  \n",
       "0                0             0             0             0             0  \n",
       "1                0             0             0             0             0  \n",
       "2                0             0             0             0             0  \n",
       "3                0             0             0             0             0  \n",
       "4                0             0             0             0             0  \n",
       "...            ...           ...           ...           ...           ...  \n",
       "9995             0             0             0             0             0  \n",
       "9996             0             0             0             0             0  \n",
       "9997             0             0             0             0             0  \n",
       "9998             0             0             0             0             0  \n",
       "9999             0             0             0             0             0  \n",
       "\n",
       "[10000 rows x 572 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_dummies = pd.get_dummies(df['AIRLINE'], prefix = 'AIRLINE')\n",
    "or_airport_dummies = pd.get_dummies(df['ORIGIN_AIRPORT'], prefix = 'OR_AIR')\n",
    "dest_airport_dummies = pd.get_dummies(df['DESTINATION_AIRPORT'], prefix = 'DEST_AIR')\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df,\n",
    "    right=airline_dummies,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df,\n",
    "    right=or_airport_dummies,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "df = pd.merge(\n",
    "    left=df,\n",
    "    right=dest_airport_dummies,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "df.drop(columns=['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23bee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'TARGET_1'\n",
    "\n",
    "X = df.loc[:, ~df.columns.isin(['ELAPSED_TIME', 'SCHEDULED_ARRIVAL', 'CANCELLED', 'TARGET_1', 'TARGET_2', 'TARGET_3'])]\n",
    "y = df.loc[:, target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2be226bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d6742",
   "metadata": {},
   "source": [
    "## Train the model (MPLRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6a4a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5992.63553789\n",
      "Iteration 2, loss = 4673.02925629\n",
      "Iteration 3, loss = 3857.16390762\n",
      "Iteration 4, loss = 3570.69105279\n",
      "Iteration 5, loss = 3511.95778552\n",
      "Iteration 6, loss = 3507.30000308\n",
      "Iteration 7, loss = 3495.71279981\n",
      "Iteration 8, loss = 3490.72813422\n",
      "Iteration 9, loss = 3484.36957127\n",
      "Iteration 10, loss = 3482.13913976\n",
      "Iteration 11, loss = 3479.24952611\n",
      "Iteration 12, loss = 3468.48543717\n",
      "Iteration 13, loss = 3470.25914318\n",
      "Iteration 14, loss = 3467.49845164\n",
      "Iteration 15, loss = 3452.26561801\n",
      "Iteration 16, loss = 3467.05895604\n",
      "Iteration 17, loss = 3460.55316890\n",
      "Iteration 18, loss = 3452.10028450\n",
      "Iteration 19, loss = 3434.37148729\n",
      "Iteration 20, loss = 3435.68845198\n",
      "Iteration 21, loss = 3431.69501499\n",
      "Iteration 22, loss = 3429.31098219\n",
      "Iteration 23, loss = 3424.75883080\n",
      "Iteration 24, loss = 3417.14457934\n",
      "Iteration 25, loss = 3410.99514168\n",
      "Iteration 26, loss = 3407.24311280\n",
      "Iteration 27, loss = 3415.18488846\n",
      "Iteration 28, loss = 3416.82563999\n",
      "Iteration 29, loss = 3408.14141526\n",
      "Iteration 30, loss = 3412.15436643\n",
      "Iteration 31, loss = 3427.11731167\n",
      "Iteration 32, loss = 3394.26201615\n",
      "Iteration 33, loss = 3391.03947458\n",
      "Iteration 34, loss = 3394.18829448\n",
      "Iteration 35, loss = 3389.25708200\n",
      "Iteration 36, loss = 3387.93672534\n",
      "Iteration 37, loss = 3384.20365114\n",
      "Iteration 38, loss = 3386.28564874\n",
      "Iteration 39, loss = 3381.47119049\n",
      "Iteration 40, loss = 3377.61395247\n",
      "Iteration 41, loss = 3385.73302374\n",
      "Iteration 42, loss = 3368.97875620\n",
      "Iteration 43, loss = 3375.19969069\n",
      "Iteration 44, loss = 3367.42789729\n",
      "Iteration 45, loss = 3371.94742446\n",
      "Iteration 46, loss = 3390.24353912\n",
      "Iteration 47, loss = 3362.25530302\n",
      "Iteration 48, loss = 3363.07357269\n",
      "Iteration 49, loss = 3360.21195913\n",
      "Iteration 50, loss = 3363.54002187\n",
      "Iteration 51, loss = 3360.19872385\n",
      "Iteration 52, loss = 3348.62884689\n",
      "Iteration 53, loss = 3348.64938644\n",
      "Iteration 54, loss = 3362.70242293\n",
      "Iteration 55, loss = 3346.09020775\n",
      "Iteration 56, loss = 3353.18638978\n",
      "Iteration 57, loss = 3347.25750278\n",
      "Iteration 58, loss = 3343.38930924\n",
      "Iteration 59, loss = 3334.35249532\n",
      "Iteration 60, loss = 3341.92325369\n",
      "Iteration 61, loss = 3330.78651565\n",
      "Iteration 62, loss = 3335.75619740\n",
      "Iteration 63, loss = 3345.57457306\n",
      "Iteration 64, loss = 3337.34136735\n",
      "Iteration 65, loss = 3335.60206210\n",
      "Iteration 66, loss = 3328.77004975\n",
      "Iteration 67, loss = 3328.03566229\n",
      "Iteration 68, loss = 3322.69127519\n",
      "Iteration 69, loss = 3335.88794651\n",
      "Iteration 70, loss = 3337.20996209\n",
      "Iteration 71, loss = 3343.62984396\n",
      "Iteration 72, loss = 3331.61548645\n",
      "Iteration 73, loss = 3323.21067300\n",
      "Iteration 74, loss = 3320.30372694\n",
      "Iteration 75, loss = 3313.36392902\n",
      "Iteration 76, loss = 3306.76259574\n",
      "Iteration 77, loss = 3312.18440613\n",
      "Iteration 78, loss = 3300.81302929\n",
      "Iteration 79, loss = 3315.93042671\n",
      "Iteration 80, loss = 3315.19119298\n",
      "Iteration 81, loss = 3302.02012735\n",
      "Iteration 82, loss = 3298.34857771\n",
      "Iteration 83, loss = 3305.65989743\n",
      "Iteration 84, loss = 3298.62257538\n",
      "Iteration 85, loss = 3294.96309514\n",
      "Iteration 86, loss = 3292.80025563\n",
      "Iteration 87, loss = 3313.12351380\n",
      "Iteration 88, loss = 3293.36915636\n",
      "Iteration 89, loss = 3311.06312358\n",
      "Iteration 90, loss = 3284.15097705\n",
      "Iteration 91, loss = 3288.81399137\n",
      "Iteration 92, loss = 3281.77683397\n",
      "Iteration 93, loss = 3282.51411562\n",
      "Iteration 94, loss = 3278.14841435\n",
      "Iteration 95, loss = 3280.23921900\n",
      "Iteration 96, loss = 3281.39636133\n",
      "Iteration 97, loss = 3274.83542314\n",
      "Iteration 98, loss = 3277.35698961\n",
      "Iteration 99, loss = 3284.75053217\n",
      "Iteration 100, loss = 3275.79719688\n",
      "Iteration 101, loss = 3272.27643224\n",
      "Iteration 102, loss = 3271.19625027\n",
      "Iteration 103, loss = 3297.15860776\n",
      "Iteration 104, loss = 3276.81218082\n",
      "Iteration 105, loss = 3263.79340772\n",
      "Iteration 106, loss = 3271.67081274\n",
      "Iteration 107, loss = 3293.20170190\n",
      "Iteration 108, loss = 3271.06060108\n",
      "Iteration 109, loss = 3262.35565810\n",
      "Iteration 110, loss = 3269.82620442\n",
      "Iteration 111, loss = 3256.09900948\n",
      "Iteration 112, loss = 3271.92657435\n",
      "Iteration 113, loss = 3265.24741560\n",
      "Iteration 114, loss = 3255.28802595\n",
      "Iteration 115, loss = 3258.89043656\n",
      "Iteration 116, loss = 3253.82296386\n",
      "Iteration 117, loss = 3266.52923927\n",
      "Iteration 118, loss = 3257.83589179\n",
      "Iteration 119, loss = 3248.12949026\n",
      "Iteration 120, loss = 3254.43419703\n",
      "Iteration 121, loss = 3247.18395845\n",
      "Iteration 122, loss = 3243.32016126\n",
      "Iteration 123, loss = 3255.42769331\n",
      "Iteration 124, loss = 3260.79708768\n",
      "Iteration 125, loss = 3257.13181215\n",
      "Iteration 126, loss = 3267.99720275\n",
      "Iteration 127, loss = 3238.45111566\n",
      "Iteration 128, loss = 3238.76452839\n",
      "Iteration 129, loss = 3255.21561800\n",
      "Iteration 130, loss = 3234.99084530\n",
      "Iteration 131, loss = 3239.37929215\n",
      "Iteration 132, loss = 3243.31525067\n",
      "Iteration 133, loss = 3245.30294336\n",
      "Iteration 134, loss = 3228.54229099\n",
      "Iteration 135, loss = 3229.43308643\n",
      "Iteration 136, loss = 3221.59211042\n",
      "Iteration 137, loss = 3232.30540638\n",
      "Iteration 138, loss = 3244.13045153\n",
      "Iteration 139, loss = 3222.89382957\n",
      "Iteration 140, loss = 3226.79558708\n",
      "Iteration 141, loss = 3221.81375616\n",
      "Iteration 142, loss = 3243.94916913\n",
      "Iteration 143, loss = 3209.59551151\n",
      "Iteration 144, loss = 3224.33358552\n",
      "Iteration 145, loss = 3214.45491345\n",
      "Iteration 146, loss = 3223.26142530\n",
      "Iteration 147, loss = 3212.82608482\n",
      "Iteration 148, loss = 3210.96187374\n",
      "Iteration 149, loss = 3214.07660001\n",
      "Iteration 150, loss = 3197.28999914\n",
      "Iteration 151, loss = 3208.27064004\n",
      "Iteration 152, loss = 3201.24754538\n",
      "Iteration 153, loss = 3200.85736400\n",
      "Iteration 154, loss = 3191.89080892\n",
      "Iteration 155, loss = 3190.28207908\n",
      "Iteration 156, loss = 3194.93805942\n",
      "Iteration 157, loss = 3203.15611937\n",
      "Iteration 158, loss = 3195.43710248\n",
      "Iteration 159, loss = 3195.76812150\n",
      "Iteration 160, loss = 3212.39318627\n",
      "Iteration 161, loss = 3182.67672942\n",
      "Iteration 162, loss = 3224.23690088\n",
      "Iteration 163, loss = 3193.14516783\n",
      "Iteration 164, loss = 3180.49871094\n",
      "Iteration 165, loss = 3179.37542523\n",
      "Iteration 166, loss = 3183.84540370\n",
      "Iteration 167, loss = 3176.00854498\n",
      "Iteration 168, loss = 3174.78858220\n",
      "Iteration 169, loss = 3175.31705973\n",
      "Iteration 170, loss = 3179.41081517\n",
      "Iteration 171, loss = 3169.67576034\n",
      "Iteration 172, loss = 3176.12600829\n",
      "Iteration 173, loss = 3172.53545502\n",
      "Iteration 174, loss = 3159.83017860\n",
      "Iteration 175, loss = 3175.24824366\n",
      "Iteration 176, loss = 3201.13786648\n",
      "Iteration 177, loss = 3165.83163186\n",
      "Iteration 178, loss = 3161.78770062\n",
      "Iteration 179, loss = 3160.76900274\n",
      "Iteration 180, loss = 3172.36641019\n",
      "Iteration 181, loss = 3152.05421275\n",
      "Iteration 182, loss = 3170.59161028\n",
      "Iteration 183, loss = 3158.30064005\n",
      "Iteration 184, loss = 3176.09499900\n",
      "Iteration 185, loss = 3170.44526046\n",
      "Iteration 186, loss = 3159.37198779\n",
      "Iteration 187, loss = 3153.65317450\n",
      "Iteration 188, loss = 3151.68168894\n",
      "Iteration 189, loss = 3170.94453757\n",
      "Iteration 190, loss = 3142.23387660\n",
      "Iteration 191, loss = 3146.72766666\n",
      "Iteration 192, loss = 3136.92838621\n",
      "Iteration 193, loss = 3152.05812677\n",
      "Iteration 194, loss = 3144.12510582\n",
      "Iteration 195, loss = 3149.38317795\n",
      "Iteration 196, loss = 3130.56359662\n",
      "Iteration 197, loss = 3153.91085175\n",
      "Iteration 198, loss = 3161.83030277\n",
      "Iteration 199, loss = 3128.58462700\n",
      "Iteration 200, loss = 3145.36161616\n",
      "Iteration 201, loss = 3174.39467138\n",
      "Iteration 202, loss = 3134.38387662\n",
      "Iteration 203, loss = 3139.11658610\n",
      "Iteration 204, loss = 3143.93610420\n",
      "Iteration 205, loss = 3129.55050586\n",
      "Iteration 206, loss = 3135.59958808\n",
      "Iteration 207, loss = 3114.46579068\n",
      "Iteration 208, loss = 3127.58222807\n",
      "Iteration 209, loss = 3135.64716526\n",
      "Iteration 210, loss = 3125.07534687\n",
      "Iteration 211, loss = 3128.90443912\n",
      "Iteration 212, loss = 3116.13161537\n",
      "Iteration 213, loss = 3144.04556504\n",
      "Iteration 214, loss = 3107.67897285\n",
      "Iteration 215, loss = 3117.32403800\n",
      "Iteration 216, loss = 3117.88551651\n",
      "Iteration 217, loss = 3107.29595494\n",
      "Iteration 218, loss = 3099.90234042\n",
      "Iteration 219, loss = 3116.98306700\n",
      "Iteration 220, loss = 3116.16348696\n",
      "Iteration 221, loss = 3097.65647152\n",
      "Iteration 222, loss = 3096.12800862\n",
      "Iteration 223, loss = 3139.89718799\n",
      "Iteration 224, loss = 3105.06966956\n",
      "Iteration 225, loss = 3096.16012320\n",
      "Iteration 226, loss = 3099.98716724\n",
      "Iteration 227, loss = 3097.27711155\n",
      "Iteration 228, loss = 3086.37195637\n",
      "Iteration 229, loss = 3096.77840890\n",
      "Iteration 230, loss = 3111.77299299\n",
      "Iteration 231, loss = 3086.65671205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 232, loss = 3086.55794637\n",
      "Iteration 233, loss = 3081.67256522\n",
      "Iteration 234, loss = 3108.00717926\n",
      "Iteration 235, loss = 3109.15239549\n",
      "Iteration 236, loss = 3094.12048306\n",
      "Iteration 237, loss = 3089.07727555\n",
      "Iteration 238, loss = 3099.53657783\n",
      "Iteration 239, loss = 3087.40546305\n",
      "Iteration 240, loss = 3106.60201300\n",
      "Iteration 241, loss = 3072.94065376\n",
      "Iteration 242, loss = 3096.51482570\n",
      "Iteration 243, loss = 3103.86133683\n",
      "Iteration 244, loss = 3081.64493771\n",
      "Iteration 245, loss = 3065.32199837\n",
      "Iteration 246, loss = 3074.65564507\n",
      "Iteration 247, loss = 3091.18653150\n",
      "Iteration 248, loss = 3066.57356941\n",
      "Iteration 249, loss = 3080.89868878\n",
      "Iteration 250, loss = 3128.77023297\n",
      "Iteration 251, loss = 3084.19609243\n",
      "Iteration 252, loss = 3067.96054196\n",
      "Iteration 253, loss = 3093.37528932\n",
      "Iteration 254, loss = 3074.44979848\n",
      "Iteration 255, loss = 3070.92476041\n",
      "Iteration 256, loss = 3054.39984659\n",
      "Iteration 257, loss = 3052.99616627\n",
      "Iteration 258, loss = 3070.57163783\n",
      "Iteration 259, loss = 3086.64928139\n",
      "Iteration 260, loss = 3062.64761110\n",
      "Iteration 261, loss = 3058.10943747\n",
      "Iteration 262, loss = 3047.90234819\n",
      "Iteration 263, loss = 3052.20238322\n",
      "Iteration 264, loss = 3049.78338307\n",
      "Iteration 265, loss = 3044.95471346\n",
      "Iteration 266, loss = 3044.40694094\n",
      "Iteration 267, loss = 3052.38397683\n",
      "Iteration 268, loss = 3049.24831006\n",
      "Iteration 269, loss = 3095.95800592\n",
      "Iteration 270, loss = 3044.17855043\n",
      "Iteration 271, loss = 3054.94095505\n",
      "Iteration 272, loss = 3035.19228231\n",
      "Iteration 273, loss = 3048.50403832\n",
      "Iteration 274, loss = 3054.47616159\n",
      "Iteration 275, loss = 3037.39724108\n",
      "Iteration 276, loss = 3043.38727025\n",
      "Iteration 277, loss = 3054.49846606\n",
      "Iteration 278, loss = 3047.84690372\n",
      "Iteration 279, loss = 3036.45954387\n",
      "Iteration 280, loss = 3058.47543644\n",
      "Iteration 281, loss = 3034.30702023\n",
      "Iteration 282, loss = 3042.43311616\n",
      "Iteration 283, loss = 3036.37428023\n",
      "Iteration 284, loss = 3043.11707236\n",
      "Iteration 285, loss = 3025.78900977\n",
      "Iteration 286, loss = 3029.43258548\n",
      "Iteration 287, loss = 3021.34270776\n",
      "Iteration 288, loss = 3026.55341750\n",
      "Iteration 289, loss = 3013.21834066\n",
      "Iteration 290, loss = 3024.31296203\n",
      "Iteration 291, loss = 3018.53182220\n",
      "Iteration 292, loss = 3031.98919193\n",
      "Iteration 293, loss = 3014.58516128\n",
      "Iteration 294, loss = 3002.15317934\n",
      "Iteration 295, loss = 3024.14908233\n",
      "Iteration 296, loss = 3014.37149208\n",
      "Iteration 297, loss = 3041.43065184\n",
      "Iteration 298, loss = 3033.73812359\n",
      "Iteration 299, loss = 3010.92992618\n",
      "Iteration 300, loss = 3033.37882275\n",
      "Iteration 301, loss = 3008.77563458\n",
      "Iteration 302, loss = 3011.77860105\n",
      "Iteration 303, loss = 3007.69703293\n",
      "Iteration 304, loss = 3001.33292747\n",
      "Iteration 305, loss = 2996.01455443\n",
      "Iteration 306, loss = 3014.36797066\n",
      "Iteration 307, loss = 3065.77719954\n",
      "Iteration 308, loss = 3034.87085695\n",
      "Iteration 309, loss = 2999.55874878\n",
      "Iteration 310, loss = 2987.52966668\n",
      "Iteration 311, loss = 3001.15781562\n",
      "Iteration 312, loss = 3001.39027183\n",
      "Iteration 313, loss = 3057.38468957\n",
      "Iteration 314, loss = 3001.61817670\n",
      "Iteration 315, loss = 2992.24602984\n",
      "Iteration 316, loss = 2981.32436732\n",
      "Iteration 317, loss = 2976.99637954\n",
      "Iteration 318, loss = 2990.09505710\n",
      "Iteration 319, loss = 2985.18079822\n",
      "Iteration 320, loss = 2987.94597795\n",
      "Iteration 321, loss = 2995.34186871\n",
      "Iteration 322, loss = 3048.08074826\n",
      "Iteration 323, loss = 2978.10973076\n",
      "Iteration 324, loss = 2970.94801987\n",
      "Iteration 325, loss = 2972.25474043\n",
      "Iteration 326, loss = 2967.59296458\n",
      "Iteration 327, loss = 2983.71709231\n",
      "Iteration 328, loss = 2984.34946612\n",
      "Iteration 329, loss = 2967.90759398\n",
      "Iteration 330, loss = 2972.57354963\n",
      "Iteration 331, loss = 2971.05675307\n",
      "Iteration 332, loss = 2967.66480307\n",
      "Iteration 333, loss = 2968.10100779\n",
      "Iteration 334, loss = 2985.06541221\n",
      "Iteration 335, loss = 2964.71533332\n",
      "Iteration 336, loss = 2962.92949542\n",
      "Iteration 337, loss = 2966.52828511\n",
      "Iteration 338, loss = 2981.45295977\n",
      "Iteration 339, loss = 2966.14168970\n",
      "Iteration 340, loss = 3000.12775629\n",
      "Iteration 341, loss = 2992.18189657\n",
      "Iteration 342, loss = 2979.81640107\n",
      "Iteration 343, loss = 2965.22060707\n",
      "Iteration 344, loss = 2952.86731369\n",
      "Iteration 345, loss = 2968.14935727\n",
      "Iteration 346, loss = 2954.14603550\n",
      "Iteration 347, loss = 2967.65176832\n",
      "Iteration 348, loss = 2954.98762392\n",
      "Iteration 349, loss = 2951.60132348\n",
      "Iteration 350, loss = 2953.97564144\n",
      "Iteration 351, loss = 2956.74114917\n",
      "Iteration 352, loss = 2977.23105997\n",
      "Iteration 353, loss = 2946.82788559\n",
      "Iteration 354, loss = 2941.52012957\n",
      "Iteration 355, loss = 2948.51797305\n",
      "Iteration 356, loss = 2954.47455452\n",
      "Iteration 357, loss = 3019.95409668\n",
      "Iteration 358, loss = 2959.23506862\n",
      "Iteration 359, loss = 2937.19474879\n",
      "Iteration 360, loss = 2954.73303121\n",
      "Iteration 361, loss = 2973.61646058\n",
      "Iteration 362, loss = 2933.50510258\n",
      "Iteration 363, loss = 2936.84771862\n",
      "Iteration 364, loss = 2935.97010823\n",
      "Iteration 365, loss = 2954.73032480\n",
      "Iteration 366, loss = 2933.43122766\n",
      "Iteration 367, loss = 2933.02286072\n",
      "Iteration 368, loss = 2921.47806397\n",
      "Iteration 369, loss = 2936.78621189\n",
      "Iteration 370, loss = 2956.26737106\n",
      "Iteration 371, loss = 2943.58161659\n",
      "Iteration 372, loss = 2934.30738132\n",
      "Iteration 373, loss = 2936.40612383\n",
      "Iteration 374, loss = 2928.50574243\n",
      "Iteration 375, loss = 2930.11709586\n",
      "Iteration 376, loss = 2948.23201063\n",
      "Iteration 377, loss = 2931.45571377\n",
      "Iteration 378, loss = 2919.90695735\n",
      "Iteration 379, loss = 2940.39337696\n",
      "Iteration 380, loss = 2932.72718542\n",
      "Iteration 381, loss = 2942.72040614\n",
      "Iteration 382, loss = 2906.71782884\n",
      "Iteration 383, loss = 2919.70703165\n",
      "Iteration 384, loss = 2922.19457783\n",
      "Iteration 385, loss = 2948.03149343\n",
      "Iteration 386, loss = 2967.60042753\n",
      "Iteration 387, loss = 2925.79946236\n",
      "Iteration 388, loss = 2911.07764247\n",
      "Iteration 389, loss = 2912.95919143\n",
      "Iteration 390, loss = 2895.41963061\n",
      "Iteration 391, loss = 2904.33970902\n",
      "Iteration 392, loss = 2930.41999811\n",
      "Iteration 393, loss = 2917.98519238\n",
      "Iteration 394, loss = 2908.66899361\n",
      "Iteration 395, loss = 2904.50019687\n",
      "Iteration 396, loss = 2896.19053309\n",
      "Iteration 397, loss = 2902.93811683\n",
      "Iteration 398, loss = 2899.61826203\n",
      "Iteration 399, loss = 2887.28095329\n",
      "Iteration 400, loss = 2943.09652993\n",
      "Iteration 401, loss = 2914.18369310\n",
      "Iteration 402, loss = 2926.60300625\n",
      "Iteration 403, loss = 2913.82430365\n",
      "Iteration 404, loss = 2901.20888960\n",
      "Iteration 405, loss = 2895.19119924\n",
      "Iteration 406, loss = 2909.60543973\n",
      "Iteration 407, loss = 2885.83025067\n",
      "Iteration 408, loss = 2896.27308620\n",
      "Iteration 409, loss = 2888.79479777\n",
      "Iteration 410, loss = 2900.25716519\n",
      "Iteration 411, loss = 2896.65505184\n",
      "Iteration 412, loss = 2889.69449211\n",
      "Iteration 413, loss = 2887.81076582\n",
      "Iteration 414, loss = 2888.51239541\n",
      "Iteration 415, loss = 2888.12720948\n",
      "Iteration 416, loss = 2879.00478574\n",
      "Iteration 417, loss = 2898.18868198\n",
      "Iteration 418, loss = 2929.51786790\n",
      "Iteration 419, loss = 2876.26162148\n",
      "Iteration 420, loss = 2871.26503920\n",
      "Iteration 421, loss = 2889.75710967\n",
      "Iteration 422, loss = 2926.32906988\n",
      "Iteration 423, loss = 2882.21101810\n",
      "Iteration 424, loss = 2868.45318930\n",
      "Iteration 425, loss = 2867.56292073\n",
      "Iteration 426, loss = 2882.74434836\n",
      "Iteration 427, loss = 2903.33888215\n",
      "Iteration 428, loss = 2909.54752954\n",
      "Iteration 429, loss = 2870.10594544\n",
      "Iteration 430, loss = 2863.05866133\n",
      "Iteration 431, loss = 2865.18124290\n",
      "Iteration 432, loss = 2892.92039081\n",
      "Iteration 433, loss = 2874.63006970\n",
      "Iteration 434, loss = 2879.38937950\n",
      "Iteration 435, loss = 2863.56312584\n",
      "Iteration 436, loss = 2860.97652180\n",
      "Iteration 437, loss = 2872.66439522\n",
      "Iteration 438, loss = 2861.19668730\n",
      "Iteration 439, loss = 2855.48559521\n",
      "Iteration 440, loss = 2869.58089059\n",
      "Iteration 441, loss = 2865.18035782\n",
      "Iteration 442, loss = 2901.31596039\n",
      "Iteration 443, loss = 2868.95004613\n",
      "Iteration 444, loss = 2871.18451324\n",
      "Iteration 445, loss = 2916.48400210\n",
      "Iteration 446, loss = 2892.75987392\n",
      "Iteration 447, loss = 2863.62886631\n",
      "Iteration 448, loss = 2851.33502500\n",
      "Iteration 449, loss = 2850.28445872\n",
      "Iteration 450, loss = 2869.26440636\n",
      "Iteration 451, loss = 2875.55323257\n",
      "Iteration 452, loss = 2869.26328597\n",
      "Iteration 453, loss = 2849.99865154\n",
      "Iteration 454, loss = 2838.07809197\n",
      "Iteration 455, loss = 2875.94298175\n",
      "Iteration 456, loss = 2862.14823142\n",
      "Iteration 457, loss = 2853.96929769\n",
      "Iteration 458, loss = 2826.94568863\n",
      "Iteration 459, loss = 2848.04066960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 460, loss = 2841.12979470\n",
      "Iteration 461, loss = 2835.08469023\n",
      "Iteration 462, loss = 2906.58131480\n",
      "Iteration 463, loss = 2863.24633048\n",
      "Iteration 464, loss = 2834.21698629\n",
      "Iteration 465, loss = 2853.62624637\n",
      "Iteration 466, loss = 2823.84365439\n",
      "Iteration 467, loss = 2828.06546666\n",
      "Iteration 468, loss = 2824.43546769\n",
      "Iteration 469, loss = 2850.75875721\n",
      "Iteration 470, loss = 2817.12519946\n",
      "Iteration 471, loss = 2826.29229754\n",
      "Iteration 472, loss = 2852.55872886\n",
      "Iteration 473, loss = 2820.48366784\n",
      "Iteration 474, loss = 2842.80372304\n",
      "Iteration 475, loss = 2857.85738496\n",
      "Iteration 476, loss = 2855.60914888\n",
      "Iteration 477, loss = 2830.89908719\n",
      "Iteration 478, loss = 2823.48684220\n",
      "Iteration 479, loss = 2836.05146613\n",
      "Iteration 480, loss = 2817.05069457\n",
      "Iteration 481, loss = 2819.13867142\n",
      "Iteration 482, loss = 2841.42750714\n",
      "Iteration 483, loss = 2812.52150713\n",
      "Iteration 484, loss = 2810.75987196\n",
      "Iteration 485, loss = 2812.51338264\n",
      "Iteration 486, loss = 2831.50552448\n",
      "Iteration 487, loss = 2824.71089724\n",
      "Iteration 488, loss = 2807.74229508\n",
      "Iteration 489, loss = 2814.32581954\n",
      "Iteration 490, loss = 2803.85265588\n",
      "Iteration 491, loss = 2822.19379262\n",
      "Iteration 492, loss = 2804.78601153\n",
      "Iteration 493, loss = 2792.56212416\n",
      "Iteration 494, loss = 2825.34635490\n",
      "Iteration 495, loss = 2799.24712830\n",
      "Iteration 496, loss = 2816.01276037\n",
      "Iteration 497, loss = 2798.13702527\n",
      "Iteration 498, loss = 2843.25638391\n",
      "Iteration 499, loss = 2801.76203808\n",
      "Iteration 500, loss = 2788.24621614\n",
      "Iteration 1, loss = 7350.36011767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wille\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 7184.94183814\n",
      "Iteration 3, loss = 6985.30420596\n",
      "Iteration 4, loss = 6761.27637315\n",
      "Iteration 5, loss = 6549.03809724\n",
      "Iteration 6, loss = 6376.36274891\n",
      "Iteration 7, loss = 6240.19171481\n",
      "Iteration 8, loss = 6118.98514214\n",
      "Iteration 9, loss = 5997.93891427\n",
      "Iteration 10, loss = 5872.33874190\n",
      "Iteration 11, loss = 5738.38126525\n",
      "Iteration 12, loss = 5590.82484752\n",
      "Iteration 13, loss = 5435.69188787\n",
      "Iteration 14, loss = 5261.15132225\n",
      "Iteration 15, loss = 5079.39843831\n",
      "Iteration 16, loss = 4885.43972187\n",
      "Iteration 17, loss = 4688.75846771\n",
      "Iteration 18, loss = 4495.07390743\n",
      "Iteration 19, loss = 4307.73178186\n",
      "Iteration 20, loss = 4128.37692001\n",
      "Iteration 21, loss = 3961.00630153\n",
      "Iteration 22, loss = 3808.23751294\n",
      "Iteration 23, loss = 3669.85502929\n",
      "Iteration 24, loss = 3547.69480618\n",
      "Iteration 25, loss = 3441.02801972\n",
      "Iteration 26, loss = 3349.08421282\n",
      "Iteration 27, loss = 3266.66195797\n",
      "Iteration 28, loss = 3202.85749940\n",
      "Iteration 29, loss = 3151.14306412\n",
      "Iteration 30, loss = 3104.90416301\n",
      "Iteration 31, loss = 3070.38866127\n",
      "Iteration 32, loss = 3038.54233958\n",
      "Iteration 33, loss = 3012.32675317\n",
      "Iteration 34, loss = 2992.94147948\n",
      "Iteration 35, loss = 2972.91874423\n",
      "Iteration 36, loss = 2957.80831183\n",
      "Iteration 37, loss = 2947.55979940\n",
      "Iteration 38, loss = 2933.80757686\n",
      "Iteration 39, loss = 2921.73464544\n",
      "Iteration 40, loss = 2911.91513383\n",
      "Iteration 41, loss = 2902.43227526\n",
      "Iteration 42, loss = 2892.22209403\n",
      "Iteration 43, loss = 2879.13545222\n",
      "Iteration 44, loss = 2872.71045281\n",
      "Iteration 45, loss = 2864.18736514\n",
      "Iteration 46, loss = 2854.30053288\n",
      "Iteration 47, loss = 2847.03063087\n",
      "Iteration 48, loss = 2839.03355705\n",
      "Iteration 49, loss = 2825.83045323\n",
      "Iteration 50, loss = 2817.51224227\n",
      "Iteration 51, loss = 2808.82266442\n",
      "Iteration 52, loss = 2798.30141739\n",
      "Iteration 53, loss = 2793.94364444\n",
      "Iteration 54, loss = 2780.16179666\n",
      "Iteration 55, loss = 2772.28886568\n",
      "Iteration 56, loss = 2762.59788487\n",
      "Iteration 57, loss = 2754.81592476\n",
      "Iteration 58, loss = 2746.83526252\n",
      "Iteration 59, loss = 2737.99990368\n",
      "Iteration 60, loss = 2730.83931546\n",
      "Iteration 61, loss = 2722.26242230\n",
      "Iteration 62, loss = 2711.83901597\n",
      "Iteration 63, loss = 2703.94905093\n",
      "Iteration 64, loss = 2692.50245102\n",
      "Iteration 65, loss = 2683.95747629\n",
      "Iteration 66, loss = 2674.30586885\n",
      "Iteration 67, loss = 2667.19569699\n",
      "Iteration 68, loss = 2656.30146411\n",
      "Iteration 69, loss = 2648.54207944\n",
      "Iteration 70, loss = 2637.55156835\n",
      "Iteration 71, loss = 2631.67514919\n",
      "Iteration 72, loss = 2618.93624843\n",
      "Iteration 73, loss = 2615.44639776\n",
      "Iteration 74, loss = 2605.29711534\n",
      "Iteration 75, loss = 2595.17201472\n",
      "Iteration 76, loss = 2589.87844197\n",
      "Iteration 77, loss = 2577.74452734\n",
      "Iteration 78, loss = 2569.06248449\n",
      "Iteration 79, loss = 2560.98179974\n",
      "Iteration 80, loss = 2550.46857747\n",
      "Iteration 81, loss = 2543.92204610\n",
      "Iteration 82, loss = 2538.51186002\n",
      "Iteration 83, loss = 2528.99494339\n",
      "Iteration 84, loss = 2519.14803851\n",
      "Iteration 85, loss = 2511.65225927\n",
      "Iteration 86, loss = 2501.58863166\n",
      "Iteration 87, loss = 2496.94007879\n",
      "Iteration 88, loss = 2487.48880094\n",
      "Iteration 89, loss = 2478.46467651\n",
      "Iteration 90, loss = 2469.00885868\n",
      "Iteration 91, loss = 2461.67917054\n",
      "Iteration 92, loss = 2457.94875760\n",
      "Iteration 93, loss = 2446.31846837\n",
      "Iteration 94, loss = 2438.78420939\n",
      "Iteration 95, loss = 2432.15786762\n",
      "Iteration 96, loss = 2424.19245589\n",
      "Iteration 97, loss = 2414.31387827\n",
      "Iteration 98, loss = 2408.19267109\n",
      "Iteration 99, loss = 2398.63409457\n",
      "Iteration 100, loss = 2388.44953612\n",
      "Iteration 101, loss = 2384.89627069\n",
      "Iteration 102, loss = 2378.04376427\n",
      "Iteration 103, loss = 2369.35095653\n",
      "Iteration 104, loss = 2360.23671052\n",
      "Iteration 105, loss = 2353.49017048\n",
      "Iteration 106, loss = 2345.69247093\n",
      "Iteration 107, loss = 2338.38720046\n",
      "Iteration 108, loss = 2330.85183446\n",
      "Iteration 109, loss = 2326.68449478\n",
      "Iteration 110, loss = 2316.95666599\n",
      "Iteration 111, loss = 2312.63502163\n",
      "Iteration 112, loss = 2302.49186860\n",
      "Iteration 113, loss = 2297.43127481\n",
      "Iteration 114, loss = 2290.07003424\n",
      "Iteration 115, loss = 2281.56805656\n",
      "Iteration 116, loss = 2278.03963747\n",
      "Iteration 117, loss = 2272.54044317\n",
      "Iteration 118, loss = 2260.55823110\n",
      "Iteration 119, loss = 2259.66138395\n",
      "Iteration 120, loss = 2250.58498888\n",
      "Iteration 121, loss = 2242.26160619\n",
      "Iteration 122, loss = 2232.23715370\n",
      "Iteration 123, loss = 2229.31020173\n",
      "Iteration 124, loss = 2223.86763837\n",
      "Iteration 125, loss = 2214.49858555\n",
      "Iteration 126, loss = 2210.18276115\n",
      "Iteration 127, loss = 2206.13335362\n",
      "Iteration 128, loss = 2196.88329761\n",
      "Iteration 129, loss = 2191.90055637\n",
      "Iteration 130, loss = 2184.16321570\n",
      "Iteration 131, loss = 2175.92210344\n",
      "Iteration 132, loss = 2171.79921278\n",
      "Iteration 133, loss = 2165.78750984\n",
      "Iteration 134, loss = 2162.84410941\n",
      "Iteration 135, loss = 2159.41029085\n",
      "Iteration 136, loss = 2147.45627748\n",
      "Iteration 137, loss = 2142.31338294\n",
      "Iteration 138, loss = 2139.71988616\n",
      "Iteration 139, loss = 2132.75896898\n",
      "Iteration 140, loss = 2125.23801155\n",
      "Iteration 141, loss = 2120.65656156\n",
      "Iteration 142, loss = 2116.16141304\n",
      "Iteration 143, loss = 2109.35405214\n",
      "Iteration 144, loss = 2104.91658985\n",
      "Iteration 145, loss = 2104.80158475\n",
      "Iteration 146, loss = 2090.21592637\n",
      "Iteration 147, loss = 2085.36672894\n",
      "Iteration 148, loss = 2080.97881233\n",
      "Iteration 149, loss = 2073.78604582\n",
      "Iteration 150, loss = 2068.22841306\n",
      "Iteration 151, loss = 2063.92839277\n",
      "Iteration 152, loss = 2058.00998745\n",
      "Iteration 153, loss = 2052.46596000\n",
      "Iteration 154, loss = 2051.76801093\n",
      "Iteration 155, loss = 2046.72187374\n",
      "Iteration 156, loss = 2040.86935734\n",
      "Iteration 157, loss = 2037.54462712\n",
      "Iteration 158, loss = 2027.74874358\n",
      "Iteration 159, loss = 2023.18666247\n",
      "Iteration 160, loss = 2019.28799634\n",
      "Iteration 161, loss = 2016.47375291\n",
      "Iteration 162, loss = 2011.95749161\n",
      "Iteration 163, loss = 2005.06654222\n",
      "Iteration 164, loss = 1999.43617635\n",
      "Iteration 165, loss = 1994.05357622\n",
      "Iteration 166, loss = 1989.77138660\n",
      "Iteration 167, loss = 1983.12910606\n",
      "Iteration 168, loss = 1981.11622756\n",
      "Iteration 169, loss = 1975.98531050\n",
      "Iteration 170, loss = 1975.23594636\n",
      "Iteration 171, loss = 1970.36438150\n",
      "Iteration 172, loss = 1962.91957034\n",
      "Iteration 173, loss = 1959.80569087\n",
      "Iteration 174, loss = 1952.98184558\n",
      "Iteration 175, loss = 1946.21598797\n",
      "Iteration 176, loss = 1946.68816638\n",
      "Iteration 177, loss = 1937.44375998\n",
      "Iteration 178, loss = 1933.55096287\n",
      "Iteration 179, loss = 1927.80180801\n",
      "Iteration 180, loss = 1929.43393731\n",
      "Iteration 181, loss = 1923.88424562\n",
      "Iteration 182, loss = 1922.99183292\n",
      "Iteration 183, loss = 1917.03595997\n",
      "Iteration 184, loss = 1913.51192145\n",
      "Iteration 185, loss = 1901.63652937\n",
      "Iteration 186, loss = 1901.20111144\n",
      "Iteration 187, loss = 1900.65056651\n",
      "Iteration 188, loss = 1895.22950339\n",
      "Iteration 189, loss = 1890.45631353\n",
      "Iteration 190, loss = 1889.21186008\n",
      "Iteration 191, loss = 1885.21511526\n",
      "Iteration 192, loss = 1882.32908020\n",
      "Iteration 193, loss = 1874.19153675\n",
      "Iteration 194, loss = 1869.18212447\n",
      "Iteration 195, loss = 1867.26730615\n",
      "Iteration 196, loss = 1862.88223965\n",
      "Iteration 197, loss = 1861.43163151\n",
      "Iteration 198, loss = 1857.90144856\n",
      "Iteration 199, loss = 1857.23161876\n",
      "Iteration 200, loss = 1850.81378055\n",
      "Iteration 201, loss = 1845.27455661\n",
      "Iteration 202, loss = 1842.64020771\n",
      "Iteration 203, loss = 1841.95985323\n",
      "Iteration 204, loss = 1837.69098138\n",
      "Iteration 205, loss = 1829.41770411\n",
      "Iteration 206, loss = 1829.48165669\n",
      "Iteration 207, loss = 1828.87456540\n",
      "Iteration 208, loss = 1821.55931903\n",
      "Iteration 209, loss = 1817.08787052\n",
      "Iteration 210, loss = 1814.33045303\n",
      "Iteration 211, loss = 1811.99741174\n",
      "Iteration 212, loss = 1803.70971975\n",
      "Iteration 213, loss = 1804.36951806\n",
      "Iteration 214, loss = 1802.22030502\n",
      "Iteration 215, loss = 1798.13647591\n",
      "Iteration 216, loss = 1795.24330565\n",
      "Iteration 217, loss = 1791.79294368\n",
      "Iteration 218, loss = 1785.61009662\n",
      "Iteration 219, loss = 1786.41222916\n",
      "Iteration 220, loss = 1782.45048460\n",
      "Iteration 221, loss = 1777.32239408\n",
      "Iteration 222, loss = 1775.28033398\n",
      "Iteration 223, loss = 1770.30015413\n",
      "Iteration 224, loss = 1766.15194053\n",
      "Iteration 225, loss = 1766.36553891\n",
      "Iteration 226, loss = 1762.91850633\n",
      "Iteration 227, loss = 1758.67098512\n",
      "Iteration 228, loss = 1757.98182786\n",
      "Iteration 229, loss = 1755.09897030\n",
      "Iteration 230, loss = 1753.10698201\n",
      "Iteration 231, loss = 1749.53351398\n",
      "Iteration 232, loss = 1746.91108235\n",
      "Iteration 233, loss = 1745.93552712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 234, loss = 1737.64576898\n",
      "Iteration 235, loss = 1734.41326333\n",
      "Iteration 236, loss = 1736.16614524\n",
      "Iteration 237, loss = 1733.39604499\n",
      "Iteration 238, loss = 1731.05443495\n",
      "Iteration 239, loss = 1724.76484709\n",
      "Iteration 240, loss = 1724.09061772\n",
      "Iteration 241, loss = 1720.25086173\n",
      "Iteration 242, loss = 1718.00887834\n",
      "Iteration 243, loss = 1718.94037445\n",
      "Iteration 244, loss = 1715.74785594\n",
      "Iteration 245, loss = 1710.87595491\n",
      "Iteration 246, loss = 1707.12615176\n",
      "Iteration 247, loss = 1704.19317223\n",
      "Iteration 248, loss = 1700.87122864\n",
      "Iteration 249, loss = 1696.56058087\n",
      "Iteration 250, loss = 1695.68172117\n",
      "Iteration 251, loss = 1694.12650070\n",
      "Iteration 252, loss = 1686.14970520\n",
      "Iteration 253, loss = 1683.84297903\n",
      "Iteration 254, loss = 1680.78786518\n",
      "Iteration 255, loss = 1681.23110327\n",
      "Iteration 256, loss = 1682.01329645\n",
      "Iteration 257, loss = 1679.19851179\n",
      "Iteration 258, loss = 1673.79429059\n",
      "Iteration 259, loss = 1672.08180277\n",
      "Iteration 260, loss = 1671.02520339\n",
      "Iteration 261, loss = 1666.16865868\n",
      "Iteration 262, loss = 1665.79822894\n",
      "Iteration 263, loss = 1662.94923791\n",
      "Iteration 264, loss = 1661.94894896\n",
      "Iteration 265, loss = 1657.63649804\n",
      "Iteration 266, loss = 1655.89071474\n",
      "Iteration 267, loss = 1655.98624028\n",
      "Iteration 268, loss = 1651.45600172\n",
      "Iteration 269, loss = 1654.86745598\n",
      "Iteration 270, loss = 1643.50904587\n",
      "Iteration 271, loss = 1639.57410888\n",
      "Iteration 272, loss = 1640.21487520\n",
      "Iteration 273, loss = 1634.90527898\n",
      "Iteration 274, loss = 1635.95258371\n",
      "Iteration 275, loss = 1638.09874720\n",
      "Iteration 276, loss = 1628.64335726\n",
      "Iteration 277, loss = 1627.42963253\n",
      "Iteration 278, loss = 1626.18085262\n",
      "Iteration 279, loss = 1622.41290961\n",
      "Iteration 280, loss = 1618.42113144\n",
      "Iteration 281, loss = 1621.88433035\n",
      "Iteration 282, loss = 1617.04168301\n",
      "Iteration 283, loss = 1612.06297756\n",
      "Iteration 284, loss = 1610.61782230\n",
      "Iteration 285, loss = 1610.47293329\n",
      "Iteration 286, loss = 1607.65599446\n",
      "Iteration 287, loss = 1605.40906893\n",
      "Iteration 288, loss = 1605.72480340\n",
      "Iteration 289, loss = 1599.85935436\n",
      "Iteration 290, loss = 1596.73835651\n",
      "Iteration 291, loss = 1597.51753058\n",
      "Iteration 292, loss = 1592.40540354\n",
      "Iteration 293, loss = 1590.03887711\n",
      "Iteration 294, loss = 1586.92739100\n",
      "Iteration 295, loss = 1583.07838154\n",
      "Iteration 296, loss = 1583.05078369\n",
      "Iteration 297, loss = 1579.51929181\n",
      "Iteration 298, loss = 1579.33060494\n",
      "Iteration 299, loss = 1579.45909115\n",
      "Iteration 300, loss = 1577.20977508\n",
      "Iteration 301, loss = 1572.65702650\n",
      "Iteration 302, loss = 1571.92515882\n",
      "Iteration 303, loss = 1571.40751475\n",
      "Iteration 304, loss = 1570.02608418\n",
      "Iteration 305, loss = 1566.38659609\n",
      "Iteration 306, loss = 1564.58642694\n",
      "Iteration 307, loss = 1559.58916965\n",
      "Iteration 308, loss = 1560.29759548\n",
      "Iteration 309, loss = 1559.19313064\n",
      "Iteration 310, loss = 1556.36589810\n",
      "Iteration 311, loss = 1553.51668990\n",
      "Iteration 312, loss = 1551.15646442\n",
      "Iteration 313, loss = 1545.82625309\n",
      "Iteration 314, loss = 1547.65408030\n",
      "Iteration 315, loss = 1546.37704496\n",
      "Iteration 316, loss = 1543.58011750\n",
      "Iteration 317, loss = 1537.48452143\n",
      "Iteration 318, loss = 1538.24903688\n",
      "Iteration 319, loss = 1532.78383702\n",
      "Iteration 320, loss = 1532.06068547\n",
      "Iteration 321, loss = 1531.84087386\n",
      "Iteration 322, loss = 1526.81083323\n",
      "Iteration 323, loss = 1523.91378420\n",
      "Iteration 324, loss = 1520.65579690\n",
      "Iteration 325, loss = 1521.96765712\n",
      "Iteration 326, loss = 1521.90353439\n",
      "Iteration 327, loss = 1516.41745146\n",
      "Iteration 328, loss = 1516.64655572\n",
      "Iteration 329, loss = 1514.58588089\n",
      "Iteration 330, loss = 1512.84288196\n",
      "Iteration 331, loss = 1510.73566430\n",
      "Iteration 332, loss = 1506.59625336\n",
      "Iteration 333, loss = 1505.75302962\n",
      "Iteration 334, loss = 1506.07610139\n",
      "Iteration 335, loss = 1500.68931312\n",
      "Iteration 336, loss = 1501.75091615\n",
      "Iteration 337, loss = 1501.09267559\n",
      "Iteration 338, loss = 1495.65009461\n",
      "Iteration 339, loss = 1495.12815930\n",
      "Iteration 340, loss = 1499.99322932\n",
      "Iteration 341, loss = 1484.19379367\n",
      "Iteration 342, loss = 1488.03845485\n",
      "Iteration 343, loss = 1484.95765235\n",
      "Iteration 344, loss = 1483.17798360\n",
      "Iteration 345, loss = 1483.93349222\n",
      "Iteration 346, loss = 1479.68987717\n",
      "Iteration 347, loss = 1478.82991174\n",
      "Iteration 348, loss = 1480.21669182\n",
      "Iteration 349, loss = 1476.85964861\n",
      "Iteration 350, loss = 1476.79375704\n",
      "Iteration 351, loss = 1470.99449380\n",
      "Iteration 352, loss = 1469.82224366\n",
      "Iteration 353, loss = 1460.33352309\n",
      "Iteration 354, loss = 1466.46087335\n",
      "Iteration 355, loss = 1466.78634002\n",
      "Iteration 356, loss = 1460.66890724\n",
      "Iteration 357, loss = 1458.09920332\n",
      "Iteration 358, loss = 1457.59612284\n",
      "Iteration 359, loss = 1456.10899552\n",
      "Iteration 360, loss = 1451.64923433\n",
      "Iteration 361, loss = 1452.40005743\n",
      "Iteration 362, loss = 1447.29155093\n",
      "Iteration 363, loss = 1446.14093324\n",
      "Iteration 364, loss = 1447.36878041\n",
      "Iteration 365, loss = 1440.12841211\n",
      "Iteration 366, loss = 1442.00997917\n",
      "Iteration 367, loss = 1438.33587938\n",
      "Iteration 368, loss = 1434.12925872\n",
      "Iteration 369, loss = 1439.71926809\n",
      "Iteration 370, loss = 1434.38623344\n",
      "Iteration 371, loss = 1433.62462875\n",
      "Iteration 372, loss = 1430.66403594\n",
      "Iteration 373, loss = 1432.28953085\n",
      "Iteration 374, loss = 1426.68969829\n",
      "Iteration 375, loss = 1430.21726855\n",
      "Iteration 376, loss = 1425.28909310\n",
      "Iteration 377, loss = 1426.92118394\n",
      "Iteration 378, loss = 1420.35912463\n",
      "Iteration 379, loss = 1419.89398309\n",
      "Iteration 380, loss = 1418.63627476\n",
      "Iteration 381, loss = 1416.50309595\n",
      "Iteration 382, loss = 1415.39012233\n",
      "Iteration 383, loss = 1411.17664572\n",
      "Iteration 384, loss = 1407.87892312\n",
      "Iteration 385, loss = 1408.15718774\n",
      "Iteration 386, loss = 1410.50206596\n",
      "Iteration 387, loss = 1405.76397363\n",
      "Iteration 388, loss = 1402.04280412\n",
      "Iteration 389, loss = 1397.25604015\n",
      "Iteration 390, loss = 1396.08007828\n",
      "Iteration 391, loss = 1404.73282766\n",
      "Iteration 392, loss = 1396.39912202\n",
      "Iteration 393, loss = 1397.93323251\n",
      "Iteration 394, loss = 1392.83177769\n",
      "Iteration 395, loss = 1396.22670687\n",
      "Iteration 396, loss = 1390.78537861\n",
      "Iteration 397, loss = 1387.74657891\n",
      "Iteration 398, loss = 1386.16217027\n",
      "Iteration 399, loss = 1381.28211366\n",
      "Iteration 400, loss = 1379.16266380\n",
      "Iteration 401, loss = 1374.24364771\n",
      "Iteration 402, loss = 1373.75875861\n",
      "Iteration 403, loss = 1376.77131955\n",
      "Iteration 404, loss = 1375.73540192\n",
      "Iteration 405, loss = 1371.13936656\n",
      "Iteration 406, loss = 1371.53469684\n",
      "Iteration 407, loss = 1371.07608079\n",
      "Iteration 408, loss = 1372.45740623\n",
      "Iteration 409, loss = 1369.26284896\n",
      "Iteration 410, loss = 1365.75109319\n",
      "Iteration 411, loss = 1365.19795009\n",
      "Iteration 412, loss = 1359.59758711\n",
      "Iteration 413, loss = 1360.88997893\n",
      "Iteration 414, loss = 1355.07682372\n",
      "Iteration 415, loss = 1357.99621347\n",
      "Iteration 416, loss = 1357.05728908\n",
      "Iteration 417, loss = 1346.54763533\n",
      "Iteration 418, loss = 1348.23010106\n",
      "Iteration 419, loss = 1347.83805439\n",
      "Iteration 420, loss = 1343.51747409\n",
      "Iteration 421, loss = 1344.32925616\n",
      "Iteration 422, loss = 1345.99445958\n",
      "Iteration 423, loss = 1342.05217628\n",
      "Iteration 424, loss = 1338.59766373\n",
      "Iteration 425, loss = 1341.23658435\n",
      "Iteration 426, loss = 1338.69820415\n",
      "Iteration 427, loss = 1338.03522674\n",
      "Iteration 428, loss = 1336.59603732\n",
      "Iteration 429, loss = 1340.07705671\n",
      "Iteration 430, loss = 1331.46782228\n",
      "Iteration 431, loss = 1329.34960972\n",
      "Iteration 432, loss = 1333.74241039\n",
      "Iteration 433, loss = 1330.03601310\n",
      "Iteration 434, loss = 1329.05965737\n",
      "Iteration 435, loss = 1322.13365720\n",
      "Iteration 436, loss = 1322.68457014\n",
      "Iteration 437, loss = 1323.22283956\n",
      "Iteration 438, loss = 1318.15230321\n",
      "Iteration 439, loss = 1320.16458877\n",
      "Iteration 440, loss = 1321.15369583\n",
      "Iteration 441, loss = 1313.50178288\n",
      "Iteration 442, loss = 1312.62010912\n",
      "Iteration 443, loss = 1310.10637809\n",
      "Iteration 444, loss = 1311.35892713\n",
      "Iteration 445, loss = 1307.70442288\n",
      "Iteration 446, loss = 1305.43785405\n",
      "Iteration 447, loss = 1303.21177121\n",
      "Iteration 448, loss = 1304.14119793\n",
      "Iteration 449, loss = 1302.61086518\n",
      "Iteration 450, loss = 1306.53661564\n",
      "Iteration 451, loss = 1298.24424952\n",
      "Iteration 452, loss = 1303.85876108\n",
      "Iteration 453, loss = 1296.45853790\n",
      "Iteration 454, loss = 1297.27434298\n",
      "Iteration 455, loss = 1290.49837047\n",
      "Iteration 456, loss = 1287.51236292\n",
      "Iteration 457, loss = 1290.59982673\n",
      "Iteration 458, loss = 1284.22831548\n",
      "Iteration 459, loss = 1287.79556335\n",
      "Iteration 460, loss = 1288.54772020\n",
      "Iteration 461, loss = 1283.07791531\n",
      "Iteration 462, loss = 1283.17321115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 463, loss = 1281.42880009\n",
      "Iteration 464, loss = 1280.87783605\n",
      "Iteration 465, loss = 1280.18016291\n",
      "Iteration 466, loss = 1275.65740711\n",
      "Iteration 467, loss = 1272.24859979\n",
      "Iteration 468, loss = 1273.97698761\n",
      "Iteration 469, loss = 1270.70082415\n",
      "Iteration 470, loss = 1272.00011491\n",
      "Iteration 471, loss = 1271.16158816\n",
      "Iteration 472, loss = 1265.58615998\n",
      "Iteration 473, loss = 1261.96256850\n",
      "Iteration 474, loss = 1266.97208581\n",
      "Iteration 475, loss = 1262.78555606\n",
      "Iteration 476, loss = 1261.18955355\n",
      "Iteration 477, loss = 1258.38196536\n",
      "Iteration 478, loss = 1254.66331697\n",
      "Iteration 479, loss = 1257.45713602\n",
      "Iteration 480, loss = 1252.92851573\n",
      "Iteration 481, loss = 1251.94629069\n",
      "Iteration 482, loss = 1250.52046334\n",
      "Iteration 483, loss = 1254.17321769\n",
      "Iteration 484, loss = 1253.22961508\n",
      "Iteration 485, loss = 1252.71361092\n",
      "Iteration 486, loss = 1246.51184312\n",
      "Iteration 487, loss = 1241.14141228\n",
      "Iteration 488, loss = 1239.90189792\n",
      "Iteration 489, loss = 1243.46286981\n",
      "Iteration 490, loss = 1240.39474734\n",
      "Iteration 491, loss = 1243.77388037\n",
      "Iteration 492, loss = 1236.22867034\n",
      "Iteration 493, loss = 1232.20716982\n",
      "Iteration 494, loss = 1231.97530945\n",
      "Iteration 495, loss = 1232.14779604\n",
      "Iteration 496, loss = 1232.41013854\n",
      "Iteration 497, loss = 1232.35015116\n",
      "Iteration 498, loss = 1228.96205768\n",
      "Iteration 499, loss = 1229.91546386\n",
      "Iteration 500, loss = 1223.02532430\n",
      "Iteration 501, loss = 1226.63355121\n",
      "Iteration 502, loss = 1221.32182395\n",
      "Iteration 503, loss = 1224.67088247\n",
      "Iteration 504, loss = 1219.10859692\n",
      "Iteration 505, loss = 1222.33815592\n",
      "Iteration 506, loss = 1216.49607014\n",
      "Iteration 507, loss = 1218.06872353\n",
      "Iteration 508, loss = 1214.54840719\n",
      "Iteration 509, loss = 1211.24805081\n",
      "Iteration 510, loss = 1211.15205672\n",
      "Iteration 511, loss = 1208.03801662\n",
      "Iteration 512, loss = 1207.34078650\n",
      "Iteration 513, loss = 1205.28108543\n",
      "Iteration 514, loss = 1205.58132403\n",
      "Iteration 515, loss = 1201.16786079\n",
      "Iteration 516, loss = 1205.85177174\n",
      "Iteration 517, loss = 1202.28240638\n",
      "Iteration 518, loss = 1200.40076048\n",
      "Iteration 519, loss = 1197.00600648\n",
      "Iteration 520, loss = 1205.42332879\n",
      "Iteration 521, loss = 1200.61804568\n",
      "Iteration 522, loss = 1198.63926270\n",
      "Iteration 523, loss = 1197.72904147\n",
      "Iteration 524, loss = 1197.33982165\n",
      "Iteration 525, loss = 1190.95206832\n",
      "Iteration 526, loss = 1190.35437140\n",
      "Iteration 527, loss = 1189.09656577\n",
      "Iteration 528, loss = 1190.80736971\n",
      "Iteration 529, loss = 1182.89364704\n",
      "Iteration 530, loss = 1183.43251994\n",
      "Iteration 531, loss = 1184.74603712\n",
      "Iteration 532, loss = 1179.36632423\n",
      "Iteration 533, loss = 1176.05112002\n",
      "Iteration 534, loss = 1175.84508910\n",
      "Iteration 535, loss = 1177.14043923\n",
      "Iteration 536, loss = 1174.68006340\n",
      "Iteration 537, loss = 1176.11320055\n",
      "Iteration 538, loss = 1172.05097336\n",
      "Iteration 539, loss = 1170.68566117\n",
      "Iteration 540, loss = 1171.07983818\n",
      "Iteration 541, loss = 1172.73779047\n",
      "Iteration 542, loss = 1166.92430309\n",
      "Iteration 543, loss = 1165.25287993\n",
      "Iteration 544, loss = 1161.61972171\n",
      "Iteration 545, loss = 1162.05362921\n",
      "Iteration 546, loss = 1167.21864435\n",
      "Iteration 547, loss = 1165.45220113\n",
      "Iteration 548, loss = 1161.01633561\n",
      "Iteration 549, loss = 1157.46984614\n",
      "Iteration 550, loss = 1161.69030287\n",
      "Iteration 551, loss = 1162.26714272\n",
      "Iteration 552, loss = 1154.28938372\n",
      "Iteration 553, loss = 1156.99153672\n",
      "Iteration 554, loss = 1150.48193553\n",
      "Iteration 555, loss = 1149.88661722\n",
      "Iteration 556, loss = 1147.07635620\n",
      "Iteration 557, loss = 1147.38344932\n",
      "Iteration 558, loss = 1144.22045753\n",
      "Iteration 559, loss = 1142.14892153\n",
      "Iteration 560, loss = 1144.32319660\n",
      "Iteration 561, loss = 1140.52723813\n",
      "Iteration 562, loss = 1144.71358622\n",
      "Iteration 563, loss = 1136.99750565\n",
      "Iteration 564, loss = 1134.62620989\n",
      "Iteration 565, loss = 1135.90456369\n",
      "Iteration 566, loss = 1139.27607189\n",
      "Iteration 567, loss = 1133.98580711\n",
      "Iteration 568, loss = 1134.60562776\n",
      "Iteration 569, loss = 1128.26998378\n",
      "Iteration 570, loss = 1132.84065188\n",
      "Iteration 571, loss = 1129.62836168\n",
      "Iteration 572, loss = 1129.64739675\n",
      "Iteration 573, loss = 1131.86957651\n",
      "Iteration 574, loss = 1131.05402627\n",
      "Iteration 575, loss = 1128.63713801\n",
      "Iteration 576, loss = 1125.21288077\n",
      "Iteration 577, loss = 1122.68719374\n",
      "Iteration 578, loss = 1122.64718945\n",
      "Iteration 579, loss = 1124.35959594\n",
      "Iteration 580, loss = 1118.27732832\n",
      "Iteration 581, loss = 1119.11935694\n",
      "Iteration 582, loss = 1120.98972707\n",
      "Iteration 583, loss = 1113.95590869\n",
      "Iteration 584, loss = 1116.92903893\n",
      "Iteration 585, loss = 1114.67639978\n",
      "Iteration 586, loss = 1112.20928608\n",
      "Iteration 587, loss = 1111.49852238\n",
      "Iteration 588, loss = 1115.51476254\n",
      "Iteration 589, loss = 1108.08058969\n",
      "Iteration 590, loss = 1110.07372325\n",
      "Iteration 591, loss = 1103.00593830\n",
      "Iteration 592, loss = 1103.69583112\n",
      "Iteration 593, loss = 1100.27364570\n",
      "Iteration 594, loss = 1099.06763089\n",
      "Iteration 595, loss = 1098.29627653\n",
      "Iteration 596, loss = 1095.82799234\n",
      "Iteration 597, loss = 1101.93566204\n",
      "Iteration 598, loss = 1095.41915753\n",
      "Iteration 599, loss = 1098.48777433\n",
      "Iteration 600, loss = 1096.88776981\n",
      "Iteration 601, loss = 1093.57137599\n",
      "Iteration 602, loss = 1092.89266224\n",
      "Iteration 603, loss = 1090.70047940\n",
      "Iteration 604, loss = 1092.77855462\n",
      "Iteration 605, loss = 1089.12176201\n",
      "Iteration 606, loss = 1091.85010807\n",
      "Iteration 607, loss = 1089.23759059\n",
      "Iteration 608, loss = 1084.95574822\n",
      "Iteration 609, loss = 1082.49706154\n",
      "Iteration 610, loss = 1080.49750550\n",
      "Iteration 611, loss = 1081.21168564\n",
      "Iteration 612, loss = 1080.18382111\n",
      "Iteration 613, loss = 1076.72215625\n",
      "Iteration 614, loss = 1078.06419457\n",
      "Iteration 615, loss = 1081.40359427\n",
      "Iteration 616, loss = 1075.75004158\n",
      "Iteration 617, loss = 1074.04230117\n",
      "Iteration 618, loss = 1077.84243963\n",
      "Iteration 619, loss = 1072.28862001\n",
      "Iteration 620, loss = 1069.64214047\n",
      "Iteration 621, loss = 1071.84948920\n",
      "Iteration 622, loss = 1069.82735334\n",
      "Iteration 623, loss = 1069.04364502\n",
      "Iteration 624, loss = 1066.05090029\n",
      "Iteration 625, loss = 1071.98173131\n",
      "Iteration 626, loss = 1063.64609374\n",
      "Iteration 627, loss = 1068.26326553\n",
      "Iteration 628, loss = 1064.19167555\n",
      "Iteration 629, loss = 1065.82351464\n",
      "Iteration 630, loss = 1061.12899609\n",
      "Iteration 631, loss = 1058.98026270\n",
      "Iteration 632, loss = 1058.85851706\n",
      "Iteration 633, loss = 1059.24925842\n",
      "Iteration 634, loss = 1053.63646850\n",
      "Iteration 635, loss = 1059.10487267\n",
      "Iteration 636, loss = 1051.55577898\n",
      "Iteration 637, loss = 1054.54904627\n",
      "Iteration 638, loss = 1050.87295696\n",
      "Iteration 639, loss = 1052.23966399\n",
      "Iteration 640, loss = 1044.84205070\n",
      "Iteration 641, loss = 1044.94953910\n",
      "Iteration 642, loss = 1047.59046804\n",
      "Iteration 643, loss = 1043.52826818\n",
      "Iteration 644, loss = 1043.36772126\n",
      "Iteration 645, loss = 1041.23545789\n",
      "Iteration 646, loss = 1038.11883168\n",
      "Iteration 647, loss = 1045.12248780\n",
      "Iteration 648, loss = 1037.83925844\n",
      "Iteration 649, loss = 1037.61819916\n",
      "Iteration 650, loss = 1039.90622507\n",
      "Iteration 651, loss = 1037.37566874\n",
      "Iteration 652, loss = 1035.09619000\n",
      "Iteration 653, loss = 1034.90604192\n",
      "Iteration 654, loss = 1037.04622060\n",
      "Iteration 655, loss = 1037.56484045\n",
      "Iteration 656, loss = 1030.02158555\n",
      "Iteration 657, loss = 1028.81919349\n",
      "Iteration 658, loss = 1029.80493028\n",
      "Iteration 659, loss = 1027.69730422\n",
      "Iteration 660, loss = 1026.35172285\n",
      "Iteration 661, loss = 1024.47202946\n",
      "Iteration 662, loss = 1021.33333951\n",
      "Iteration 663, loss = 1018.41848951\n",
      "Iteration 664, loss = 1017.15460606\n",
      "Iteration 665, loss = 1025.39123754\n",
      "Iteration 666, loss = 1020.71959222\n",
      "Iteration 667, loss = 1021.76234417\n",
      "Iteration 668, loss = 1013.70302662\n",
      "Iteration 669, loss = 1016.81364837\n",
      "Iteration 670, loss = 1013.71252855\n",
      "Iteration 671, loss = 1014.91316537\n",
      "Iteration 672, loss = 1009.82416435\n",
      "Iteration 673, loss = 1016.40443807\n",
      "Iteration 674, loss = 1009.47434131\n",
      "Iteration 675, loss = 1011.01059135\n",
      "Iteration 676, loss = 1011.50188896\n",
      "Iteration 677, loss = 1011.99847116\n",
      "Iteration 678, loss = 1005.72646822\n",
      "Iteration 679, loss = 1004.66272334\n",
      "Iteration 680, loss = 1002.81109134\n",
      "Iteration 681, loss = 1009.58036250\n",
      "Iteration 682, loss = 1002.13217777\n",
      "Iteration 683, loss = 1003.10126444\n",
      "Iteration 684, loss = 999.12980141\n",
      "Iteration 685, loss = 1001.31025896\n",
      "Iteration 686, loss = 999.11258044\n",
      "Iteration 687, loss = 998.23605361\n",
      "Iteration 688, loss = 993.01572959\n",
      "Iteration 689, loss = 995.90705179\n",
      "Iteration 690, loss = 998.35870252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 691, loss = 997.17155066\n",
      "Iteration 692, loss = 993.66967852\n",
      "Iteration 693, loss = 992.05931444\n",
      "Iteration 694, loss = 995.30626451\n",
      "Iteration 695, loss = 989.67902724\n",
      "Iteration 696, loss = 987.15683223\n",
      "Iteration 697, loss = 982.62587777\n",
      "Iteration 698, loss = 986.23357803\n",
      "Iteration 699, loss = 987.31586707\n",
      "Iteration 700, loss = 986.80690892\n",
      "Iteration 701, loss = 984.07342718\n",
      "Iteration 702, loss = 982.43771496\n",
      "Iteration 703, loss = 983.07028668\n",
      "Iteration 704, loss = 982.75949117\n",
      "Iteration 705, loss = 984.31151091\n",
      "Iteration 706, loss = 977.95167402\n",
      "Iteration 707, loss = 973.94605341\n",
      "Iteration 708, loss = 974.74221819\n",
      "Iteration 709, loss = 973.01849284\n",
      "Iteration 710, loss = 971.50372271\n",
      "Iteration 711, loss = 971.28901419\n",
      "Iteration 712, loss = 972.28768759\n",
      "Iteration 713, loss = 974.64990797\n",
      "Iteration 714, loss = 971.16866542\n",
      "Iteration 715, loss = 965.35112244\n",
      "Iteration 716, loss = 974.07121317\n",
      "Iteration 717, loss = 972.39965956\n",
      "Iteration 718, loss = 965.73523416\n",
      "Iteration 719, loss = 967.54587934\n",
      "Iteration 720, loss = 965.53213605\n",
      "Iteration 721, loss = 963.57571662\n",
      "Iteration 722, loss = 959.43934770\n",
      "Iteration 723, loss = 968.85290518\n",
      "Iteration 724, loss = 964.79063255\n",
      "Iteration 725, loss = 958.15234548\n",
      "Iteration 726, loss = 958.74227985\n",
      "Iteration 727, loss = 957.37875044\n",
      "Iteration 728, loss = 956.61344433\n",
      "Iteration 729, loss = 958.78626038\n",
      "Iteration 730, loss = 953.70973254\n",
      "Iteration 731, loss = 952.80559790\n",
      "Iteration 732, loss = 956.55914253\n",
      "Iteration 733, loss = 953.83840507\n",
      "Iteration 734, loss = 951.89841235\n",
      "Iteration 735, loss = 956.10843528\n",
      "Iteration 736, loss = 950.15842047\n",
      "Iteration 737, loss = 947.62627216\n",
      "Iteration 738, loss = 945.52666250\n",
      "Iteration 739, loss = 947.10463279\n",
      "Iteration 740, loss = 945.88672142\n",
      "Iteration 741, loss = 943.52703377\n",
      "Iteration 742, loss = 947.24482029\n",
      "Iteration 743, loss = 946.81770920\n",
      "Iteration 744, loss = 951.56445590\n",
      "Iteration 745, loss = 943.57069585\n",
      "Iteration 746, loss = 942.49575008\n",
      "Iteration 747, loss = 939.61050733\n",
      "Iteration 748, loss = 942.67763142\n",
      "Iteration 749, loss = 941.22878780\n",
      "Iteration 750, loss = 937.00865437\n",
      "Iteration 751, loss = 936.88328794\n",
      "Iteration 752, loss = 938.65702966\n",
      "Iteration 753, loss = 935.03231859\n",
      "Iteration 754, loss = 935.42109437\n",
      "Iteration 755, loss = 927.70914493\n",
      "Iteration 756, loss = 936.10629433\n",
      "Iteration 757, loss = 949.65403513\n",
      "Iteration 758, loss = 934.67349138\n",
      "Iteration 759, loss = 929.36296699\n",
      "Iteration 760, loss = 929.88122522\n",
      "Iteration 761, loss = 928.93632231\n",
      "Iteration 762, loss = 935.10990154\n",
      "Iteration 763, loss = 927.90414942\n",
      "Iteration 764, loss = 924.15701513\n",
      "Iteration 765, loss = 921.20657775\n",
      "Iteration 766, loss = 924.96668240\n",
      "Iteration 767, loss = 926.95178888\n",
      "Iteration 768, loss = 922.63418149\n",
      "Iteration 769, loss = 920.68371982\n",
      "Iteration 770, loss = 919.04776405\n",
      "Iteration 771, loss = 914.76611795\n",
      "Iteration 772, loss = 916.16480251\n",
      "Iteration 773, loss = 914.34969341\n",
      "Iteration 774, loss = 911.85381738\n",
      "Iteration 775, loss = 912.23508950\n",
      "Iteration 776, loss = 909.86907926\n",
      "Iteration 777, loss = 914.75541481\n",
      "Iteration 778, loss = 914.50238646\n",
      "Iteration 779, loss = 912.31877347\n",
      "Iteration 780, loss = 911.65049760\n",
      "Iteration 781, loss = 913.56966092\n",
      "Iteration 782, loss = 906.38080444\n",
      "Iteration 783, loss = 909.91595819\n",
      "Iteration 784, loss = 903.56726880\n",
      "Iteration 785, loss = 905.41644762\n",
      "Iteration 786, loss = 905.54823349\n",
      "Iteration 787, loss = 913.18767222\n",
      "Iteration 788, loss = 912.42487955\n",
      "Iteration 789, loss = 909.59114842\n",
      "Iteration 790, loss = 912.11677064\n",
      "Iteration 791, loss = 903.28591283\n",
      "Iteration 792, loss = 908.68316862\n",
      "Iteration 793, loss = 896.03871422\n",
      "Iteration 794, loss = 896.15679767\n",
      "Iteration 795, loss = 897.32046072\n",
      "Iteration 796, loss = 898.99177887\n",
      "Iteration 797, loss = 902.31923076\n",
      "Iteration 798, loss = 896.94114924\n",
      "Iteration 799, loss = 896.02888778\n",
      "Iteration 800, loss = 897.61002157\n",
      "Iteration 801, loss = 891.88244413\n",
      "Iteration 802, loss = 895.01271028\n",
      "Iteration 803, loss = 894.60276229\n",
      "Iteration 804, loss = 890.71663269\n",
      "Iteration 805, loss = 891.22937458\n",
      "Iteration 806, loss = 889.01516166\n",
      "Iteration 807, loss = 888.19985350\n",
      "Iteration 808, loss = 884.33943329\n",
      "Iteration 809, loss = 889.44246471\n",
      "Iteration 810, loss = 886.59206415\n",
      "Iteration 811, loss = 882.74219213\n",
      "Iteration 812, loss = 886.56387621\n",
      "Iteration 813, loss = 884.21121313\n",
      "Iteration 814, loss = 889.33652799\n",
      "Iteration 815, loss = 889.02510696\n",
      "Iteration 816, loss = 879.87092717\n",
      "Iteration 817, loss = 875.96725510\n",
      "Iteration 818, loss = 881.23745006\n",
      "Iteration 819, loss = 878.58189660\n",
      "Iteration 820, loss = 877.53845831\n",
      "Iteration 821, loss = 878.76627924\n",
      "Iteration 822, loss = 873.99817148\n",
      "Iteration 823, loss = 873.67566722\n",
      "Iteration 824, loss = 872.57370470\n",
      "Iteration 825, loss = 870.03358108\n",
      "Iteration 826, loss = 869.93925613\n",
      "Iteration 827, loss = 867.51169630\n",
      "Iteration 828, loss = 867.45353944\n",
      "Iteration 829, loss = 870.96175145\n",
      "Iteration 830, loss = 871.31142165\n",
      "Iteration 831, loss = 867.38513829\n",
      "Iteration 832, loss = 865.85861532\n",
      "Iteration 833, loss = 864.46190126\n",
      "Iteration 834, loss = 863.91862632\n",
      "Iteration 835, loss = 861.82590526\n",
      "Iteration 836, loss = 870.08524255\n",
      "Iteration 837, loss = 858.47173372\n",
      "Iteration 838, loss = 859.78128159\n",
      "Iteration 839, loss = 859.45812598\n",
      "Iteration 840, loss = 858.20265419\n",
      "Iteration 841, loss = 860.37946804\n",
      "Iteration 842, loss = 859.55247387\n",
      "Iteration 843, loss = 857.99037507\n",
      "Iteration 844, loss = 858.64184523\n",
      "Iteration 845, loss = 861.69237947\n",
      "Iteration 846, loss = 856.36073485\n",
      "Iteration 847, loss = 856.29308380\n",
      "Iteration 848, loss = 860.07936616\n",
      "Iteration 849, loss = 851.04297658\n",
      "Iteration 850, loss = 856.54125037\n",
      "Iteration 851, loss = 856.48854336\n",
      "Iteration 852, loss = 854.27518757\n",
      "Iteration 853, loss = 850.68650271\n",
      "Iteration 854, loss = 850.43643938\n",
      "Iteration 855, loss = 845.27628694\n",
      "Iteration 856, loss = 849.05474041\n",
      "Iteration 857, loss = 843.71477144\n",
      "Iteration 858, loss = 843.80769592\n",
      "Iteration 859, loss = 846.45315325\n",
      "Iteration 860, loss = 847.23483755\n",
      "Iteration 861, loss = 843.45715102\n",
      "Iteration 862, loss = 847.30170400\n",
      "Iteration 863, loss = 845.81861806\n",
      "Iteration 864, loss = 846.04719970\n",
      "Iteration 865, loss = 840.00744686\n",
      "Iteration 866, loss = 836.66122042\n",
      "Iteration 867, loss = 839.15686643\n",
      "Iteration 868, loss = 840.47670122\n",
      "Iteration 869, loss = 834.67370063\n",
      "Iteration 870, loss = 835.79196629\n",
      "Iteration 871, loss = 842.19289603\n",
      "Iteration 872, loss = 833.15681661\n",
      "Iteration 873, loss = 833.60424683\n",
      "Iteration 874, loss = 832.54166077\n",
      "Iteration 875, loss = 833.82746874\n",
      "Iteration 876, loss = 831.19654166\n",
      "Iteration 877, loss = 828.49532864\n",
      "Iteration 878, loss = 833.48436660\n",
      "Iteration 879, loss = 829.66776744\n",
      "Iteration 880, loss = 832.98643620\n",
      "Iteration 881, loss = 835.41398802\n",
      "Iteration 882, loss = 830.39695899\n",
      "Iteration 883, loss = 830.76127837\n",
      "Iteration 884, loss = 828.26751628\n",
      "Iteration 885, loss = 826.95712534\n",
      "Iteration 886, loss = 825.86482695\n",
      "Iteration 887, loss = 820.08164843\n",
      "Iteration 888, loss = 822.19695147\n",
      "Iteration 889, loss = 819.72755980\n",
      "Iteration 890, loss = 822.64961342\n",
      "Iteration 891, loss = 823.51332097\n",
      "Iteration 892, loss = 821.05215792\n",
      "Iteration 893, loss = 820.28417460\n",
      "Iteration 894, loss = 818.67097356\n",
      "Iteration 895, loss = 821.65371878\n",
      "Iteration 896, loss = 819.84695714\n",
      "Iteration 897, loss = 819.61954561\n",
      "Iteration 898, loss = 817.91579893\n",
      "Iteration 899, loss = 818.89768664\n",
      "Iteration 900, loss = 815.12468674\n",
      "Iteration 901, loss = 815.55108564\n",
      "Iteration 902, loss = 816.81860009\n",
      "Iteration 903, loss = 813.91332575\n",
      "Iteration 904, loss = 813.21029805\n",
      "Iteration 905, loss = 811.12849791\n",
      "Iteration 906, loss = 819.49255714\n",
      "Iteration 907, loss = 809.07587093\n",
      "Iteration 908, loss = 813.56512786\n",
      "Iteration 909, loss = 806.36076987\n",
      "Iteration 910, loss = 804.37568235\n",
      "Iteration 911, loss = 805.64507796\n",
      "Iteration 912, loss = 802.00730499\n",
      "Iteration 913, loss = 801.44829010\n",
      "Iteration 914, loss = 800.61259420\n",
      "Iteration 915, loss = 804.17858160\n",
      "Iteration 916, loss = 806.36297246\n",
      "Iteration 917, loss = 807.73763081\n",
      "Iteration 918, loss = 810.80741288\n",
      "Iteration 919, loss = 811.24972460\n",
      "Iteration 920, loss = 803.75367758\n",
      "Iteration 921, loss = 808.04355640\n",
      "Iteration 922, loss = 799.33043764\n",
      "Iteration 923, loss = 793.12199011\n",
      "Iteration 924, loss = 797.54700295\n",
      "Iteration 925, loss = 800.34043728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 926, loss = 797.27880093\n",
      "Iteration 927, loss = 795.81497026\n",
      "Iteration 928, loss = 797.02898989\n",
      "Iteration 929, loss = 794.25628617\n",
      "Iteration 930, loss = 795.43229152\n",
      "Iteration 931, loss = 792.34141046\n",
      "Iteration 932, loss = 790.56838591\n",
      "Iteration 933, loss = 794.34081263\n",
      "Iteration 934, loss = 793.41707474\n",
      "Iteration 935, loss = 789.82889312\n",
      "Iteration 936, loss = 788.69671618\n",
      "Iteration 937, loss = 792.31637818\n",
      "Iteration 938, loss = 791.55441057\n",
      "Iteration 939, loss = 788.51684126\n",
      "Iteration 940, loss = 785.87791655\n",
      "Iteration 941, loss = 785.49588831\n",
      "Iteration 942, loss = 780.82036652\n",
      "Iteration 943, loss = 789.69715305\n",
      "Iteration 944, loss = 785.55567721\n",
      "Iteration 945, loss = 780.36416737\n",
      "Iteration 946, loss = 785.41942702\n",
      "Iteration 947, loss = 789.00702486\n",
      "Iteration 948, loss = 787.09234839\n",
      "Iteration 949, loss = 782.40564794\n",
      "Iteration 950, loss = 784.92592832\n",
      "Iteration 951, loss = 787.91265226\n",
      "Iteration 952, loss = 781.59141348\n",
      "Iteration 953, loss = 780.70512422\n",
      "Iteration 954, loss = 777.91914519\n",
      "Iteration 955, loss = 776.90786322\n",
      "Iteration 956, loss = 781.44586551\n",
      "Iteration 957, loss = 783.19545118\n",
      "Iteration 958, loss = 777.44097932\n",
      "Iteration 959, loss = 776.58625169\n",
      "Iteration 960, loss = 776.45801995\n",
      "Iteration 961, loss = 775.25911957\n",
      "Iteration 962, loss = 773.65603345\n",
      "Iteration 963, loss = 771.70421970\n",
      "Iteration 964, loss = 767.94170651\n",
      "Iteration 965, loss = 776.49877955\n",
      "Iteration 966, loss = 775.08732132\n",
      "Iteration 967, loss = 771.61307290\n",
      "Iteration 968, loss = 772.93699023\n",
      "Iteration 969, loss = 774.58191373\n",
      "Iteration 970, loss = 773.69366453\n",
      "Iteration 971, loss = 770.58597332\n",
      "Iteration 972, loss = 767.35314760\n",
      "Iteration 973, loss = 767.99287969\n",
      "Iteration 974, loss = 767.59092281\n",
      "Iteration 975, loss = 766.48531970\n",
      "Iteration 976, loss = 768.13856976\n",
      "Iteration 977, loss = 768.05812545\n",
      "Iteration 978, loss = 765.35978126\n",
      "Iteration 979, loss = 761.93316036\n",
      "Iteration 980, loss = 766.71426404\n",
      "Iteration 981, loss = 759.36354433\n",
      "Iteration 982, loss = 763.94711407\n",
      "Iteration 983, loss = 761.58839788\n",
      "Iteration 984, loss = 758.32039522\n",
      "Iteration 985, loss = 752.28539475\n",
      "Iteration 986, loss = 752.94617455\n",
      "Iteration 987, loss = 757.12508233\n",
      "Iteration 988, loss = 759.39439811\n",
      "Iteration 989, loss = 758.56820424\n",
      "Iteration 990, loss = 763.20406235\n",
      "Iteration 991, loss = 757.99737238\n",
      "Iteration 992, loss = 750.46158797\n",
      "Iteration 993, loss = 754.82597850\n",
      "Iteration 994, loss = 755.94355971\n",
      "Iteration 995, loss = 754.26911672\n",
      "Iteration 996, loss = 755.79249549\n",
      "Iteration 997, loss = 746.41084389\n",
      "Iteration 998, loss = 745.11473405\n",
      "Iteration 999, loss = 749.93258256\n",
      "Iteration 1000, loss = 745.91449648\n",
      "Iteration 1001, loss = 749.83275392\n",
      "Iteration 1002, loss = 752.73399598\n",
      "Iteration 1003, loss = 748.36508095\n",
      "Iteration 1004, loss = 746.80702682\n",
      "Iteration 1005, loss = 742.80570596\n",
      "Iteration 1006, loss = 744.02353359\n",
      "Iteration 1007, loss = 739.98918350\n",
      "Iteration 1008, loss = 744.77813494\n",
      "Iteration 1009, loss = 744.05813848\n",
      "Iteration 1010, loss = 744.15717410\n",
      "Iteration 1011, loss = 751.21444403\n",
      "Iteration 1012, loss = 750.16912480\n",
      "Iteration 1013, loss = 744.06535223\n",
      "Iteration 1014, loss = 739.16255310\n",
      "Iteration 1015, loss = 738.36749386\n",
      "Iteration 1016, loss = 743.92686934\n",
      "Iteration 1017, loss = 735.77435281\n",
      "Iteration 1018, loss = 742.07693517\n",
      "Iteration 1019, loss = 738.69940478\n",
      "Iteration 1020, loss = 737.19985413\n",
      "Iteration 1021, loss = 734.09036800\n",
      "Iteration 1022, loss = 732.26159945\n",
      "Iteration 1023, loss = 733.07570721\n",
      "Iteration 1024, loss = 730.76872981\n",
      "Iteration 1025, loss = 728.37309005\n",
      "Iteration 1026, loss = 727.44382230\n",
      "Iteration 1027, loss = 729.35897535\n",
      "Iteration 1028, loss = 735.46002910\n",
      "Iteration 1029, loss = 732.01438768\n",
      "Iteration 1030, loss = 730.71042939\n",
      "Iteration 1031, loss = 731.98079034\n",
      "Iteration 1032, loss = 736.81694935\n",
      "Iteration 1033, loss = 731.57919656\n",
      "Iteration 1034, loss = 727.41259830\n",
      "Iteration 1035, loss = 728.72106674\n",
      "Iteration 1036, loss = 731.36018817\n",
      "Iteration 1037, loss = 726.74838785\n",
      "Iteration 1038, loss = 730.90889491\n",
      "Iteration 1039, loss = 725.81550024\n",
      "Iteration 1040, loss = 724.75800909\n",
      "Iteration 1041, loss = 723.76235926\n",
      "Iteration 1042, loss = 722.76475020\n",
      "Iteration 1043, loss = 722.75857535\n",
      "Iteration 1044, loss = 720.99942395\n",
      "Iteration 1045, loss = 723.52475892\n",
      "Iteration 1046, loss = 719.13206037\n",
      "Iteration 1047, loss = 719.12217543\n",
      "Iteration 1048, loss = 723.59780546\n",
      "Iteration 1049, loss = 723.37133609\n",
      "Iteration 1050, loss = 719.72040774\n",
      "Iteration 1051, loss = 719.39563415\n",
      "Iteration 1052, loss = 716.59770486\n",
      "Iteration 1053, loss = 722.28757672\n",
      "Iteration 1054, loss = 719.68697564\n",
      "Iteration 1055, loss = 713.54332261\n",
      "Iteration 1056, loss = 713.73962752\n",
      "Iteration 1057, loss = 711.61220914\n",
      "Iteration 1058, loss = 717.21101260\n",
      "Iteration 1059, loss = 714.02296769\n",
      "Iteration 1060, loss = 712.79145908\n",
      "Iteration 1061, loss = 716.89099225\n",
      "Iteration 1062, loss = 714.46680476\n",
      "Iteration 1063, loss = 712.77937042\n",
      "Iteration 1064, loss = 711.66592290\n",
      "Iteration 1065, loss = 720.43628880\n",
      "Iteration 1066, loss = 713.95423297\n",
      "Iteration 1067, loss = 707.98781189\n",
      "Iteration 1068, loss = 709.15827659\n",
      "Iteration 1069, loss = 709.00476118\n",
      "Iteration 1070, loss = 709.66607163\n",
      "Iteration 1071, loss = 706.78594249\n",
      "Iteration 1072, loss = 706.62110473\n",
      "Iteration 1073, loss = 702.65231347\n",
      "Iteration 1074, loss = 708.77513415\n",
      "Iteration 1075, loss = 707.26905540\n",
      "Iteration 1076, loss = 704.44254521\n",
      "Iteration 1077, loss = 712.14289702\n",
      "Iteration 1078, loss = 702.22705766\n",
      "Iteration 1079, loss = 703.22485917\n",
      "Iteration 1080, loss = 703.26685156\n",
      "Iteration 1081, loss = 699.73685382\n",
      "Iteration 1082, loss = 698.74486105\n",
      "Iteration 1083, loss = 693.96953207\n",
      "Iteration 1084, loss = 704.78429551\n",
      "Iteration 1085, loss = 701.90543605\n",
      "Iteration 1086, loss = 699.79271824\n",
      "Iteration 1087, loss = 703.24271740\n",
      "Iteration 1088, loss = 698.92881242\n",
      "Iteration 1089, loss = 703.29844269\n",
      "Iteration 1090, loss = 698.01597977\n",
      "Iteration 1091, loss = 697.18848478\n",
      "Iteration 1092, loss = 695.32662169\n",
      "Iteration 1093, loss = 698.78954354\n",
      "Iteration 1094, loss = 699.25174895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "regr = MLPRegressor(random_state=1, max_iter=500, verbose=True).fit(X_train, y_train)\n",
    "regr_scaled = MLPRegressor(random_state=1, max_iter=2000, verbose=True).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d8cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for unscaled input:\n",
      "Coefficient of Determination = 0.3345749327278581\n",
      "Mean Squared Error = 7654.1445591819165\n",
      "\n",
      "Results for scaled input:\n",
      "Coefficient of Determination = -0.39239932627042795\n",
      "Mean Squared Error = 16016.267272694511\n"
     ]
    }
   ],
   "source": [
    "# Define penalty function\n",
    "def MSE(pred, target):\n",
    "    return (np.square(pred - target)).mean(axis=0)\n",
    "\n",
    "R_sq = regr.score(X_test, y_test)\n",
    "y_test_pred = regr.predict(X_test)\n",
    "mse = MSE(y_test_pred, y_test)\n",
    "\n",
    "R_sq_scaled = regr_scaled.score(X_test_scaled, y_test)\n",
    "y_test_pred_scaled = regr_scaled.predict(X_test_scaled)\n",
    "mse_scaled = MSE(y_test_pred_scaled, y_test)\n",
    "\n",
    "print(\"Results for unscaled input:\")\n",
    "print(\"Coefficient of Determination =\", R_sq)\n",
    "print(\"Mean Squared Error =\", mse)\n",
    "print(\"\")\n",
    "print(\"Results for scaled input:\")\n",
    "print(\"Coefficient of Determination =\", R_sq_scaled)\n",
    "print(\"Mean Squared Error =\", mse_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e804073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
